{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================\n",
    "#  TENSORFLOW LITE CONVERSION NOTEBOOK\n",
    "#  Konversi Model XGBoost ke TFLite untuk Android\n",
    "# =============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds untuk reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: LOAD EXISTING FEATURES AND DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 1: LOADING EXISTING FEATURES ===\")\n",
    "\n",
    "# Load combined features dari notebook sebelumnya\n",
    "try:\n",
    "    # Sesuaikan path jika perlu\n",
    "    X_train_combined, X_test_combined, y_train, y_test = joblib.load(\"features_combined.pkl\")\n",
    "    print(f\"✅ Features loaded successfully\")\n",
    "    print(f\"Train shape: {X_train_combined.shape}\")\n",
    "    print(f\"Test shape: {X_test_combined.shape}\")\n",
    "    print(f\"Label columns: {y_train.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ features_combined.pkl not found. Please run the preprocessing first.\")\n",
    "    print(\"Make sure you have run the original notebook to generate this file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load balanced data jika available\n",
    "try:\n",
    "    X_bal, y_bal = joblib.load(\"balanced_data_fixed.pkl\")\n",
    "    print(f\"✅ Balanced data loaded: {X_bal.shape}\")\n",
    "    use_balanced = True\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Balanced data not found, using original data\")\n",
    "    X_bal, y_bal = X_train_combined, y_train\n",
    "    use_balanced = False\n",
    "\n",
    "# Convert sparse matrices to dense jika diperlukan\n",
    "from scipy.sparse import issparse\n",
    "if issparse(X_bal):\n",
    "    print(\"Converting sparse matrix to dense...\")\n",
    "    X_bal_dense = X_bal.toarray().astype(np.float32)\n",
    "    X_test_dense = X_test_combined.toarray().astype(np.float32)\n",
    "else:\n",
    "    X_bal_dense = X_bal.astype(np.float32)\n",
    "    X_test_dense = X_test_combined.astype(np.float32)\n",
    "\n",
    "print(f\"Dense train shape: {X_bal_dense.shape}\")\n",
    "print(f\"Dense test shape: {X_test_dense.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: DEFINE NEURAL NETWORK ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 2: BUILDING NEURAL NETWORK MODEL ===\")\n",
    "\n",
    "def build_healthbot_model(input_dim, num_labels):\n",
    "    \"\"\"\n",
    "    Build neural network model untuk multilabel classification\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(input_dim,), dtype=tf.float32),\n",
    "\n",
    "        # Hidden layers dengan dropout untuk regularisasi\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Output layer untuk multilabel (sigmoid)\n",
    "        layers.Dense(num_labels, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile dengan optimizer dan loss untuk multilabel\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "INPUT_DIM = X_bal_dense.shape[1]\n",
    "NUM_LABELS = y_bal.shape[1]\n",
    "\n",
    "model = build_healthbot_model(INPUT_DIM, NUM_LABELS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 3: TRAINING THE MODEL ===\")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_bal_dense,\n",
    "    y_bal.values,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: EVALUATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 4: EVALUATING THE MODEL ===\")\n",
    "\n",
    "# Load best model\n",
    "model.load_weights('best_model.h5')\n",
    "\n",
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test_dense)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# F1 Scores\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "print(f\"\\nF1 Scores:\")\n",
    "print(f\"Macro F1: {f1_macro:.4f}\")\n",
    "print(f\"Micro F1: {f1_micro:.4f}\")\n",
    "\n",
    "# Compare dengan XGBoost baseline\n",
    "target_f1 = 0.95\n",
    "if f1_macro >= target_f1:\n",
    "    print(f\"✅ Target achieved! F1-score ({f1_macro:.4f}) >= {target_f1}\")\n",
    "else:\n",
    "    print(f\"⚠️ F1-score ({f1_macro:.4f}) < {target_f1}. Consider tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: EXPORT TO SAVEDMODEL FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 5: EXPORTING TO SAVEDMODEL ===\")\n",
    "\n",
    "# Create directory\n",
    "saved_model_dir = 'healthbot_classifier'\n",
    "if os.path.exists(saved_model_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(saved_model_dir)\n",
    "os.makedirs(saved_model_dir)\n",
    "\n",
    "# Save model\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "print(f\"✅ Model saved to {saved_model_dir}\")\n",
    "\n",
    "# Save label mapping\n",
    "label_mapping = {\n",
    "    'labels': y_test.columns.tolist(),\n",
    "    'index_to_label': {i: label for i, label in enumerate(y_test.columns)},\n",
    "    'label_to_index': {label: i for i, label in enumerate(y_test.columns)}\n",
    "}\n",
    "\n",
    "with open('label_mapping.json', 'w') as f:\n",
    "    json.dump(label_mapping, f, indent=2)\n",
    "print(\"✅ Label mapping saved to label_mapping.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: CONVERT TO TENSORFLOW LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 6: CONVERTING TO TENSORFLOW LITE ===\")\n",
    "\n",
    "# Load SavedModel\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "# Optimization options\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Representative dataset untuk quantization\n",
    "def representative_dataset():\n",
    "    for i in range(min(100, len(X_bal_dense))):\n",
    "        yield [X_bal_dense[i:i+1].astype(np.float32)]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "# Convert\n",
    "try:\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save TFLite model\n",
    "    tflite_path = 'healthbot_classifier.tflite'\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    model_size = os.path.getsize(tflite_path) / (1024 * 1024)  # MB\n",
    "    print(f\"✅ TFLite model saved to {tflite_path}\")\n",
    "    print(f\"Model size: {model_size:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error converting to TFLite: {e}\")\n",
    "    print(\"Trying without quantization...\")\n",
    "\n",
    "    # Fallback tanpa quantization\n",
    "    converter.optimizations = []\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    model_size = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "    print(f\"✅ TFLite model saved (no quantization): {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: CREATE PREPROCESSING ARTIFACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 7: CREATING PREPROCESSING ARTIFACTS ===\")\n",
    "\n",
    "# Load preprocessing components dari notebook asli\n",
    "try:\n",
    "    vectorizer = joblib.load(\"vectorizer_tfidf.pkl\")\n",
    "    scaler = joblib.load(\"scaler_embed.pkl\")\n",
    "\n",
    "    # Save vocabulary info untuk Android\n",
    "    vocab_info = {\n",
    "        'tfidf_vocab_size': len(vectorizer.vocabulary_),\n",
    "        'tfidf_max_features': vectorizer.max_features,\n",
    "        'tfidf_ngram_range': vectorizer.ngram_range,\n",
    "        'embedding_dim': 300,\n",
    "        'total_features': INPUT_DIM\n",
    "    }\n",
    "\n",
    "    with open('preprocessing_params.json', 'w') as f:\n",
    "        json.dump(vocab_info, f, indent=2)\n",
    "\n",
    "    print(\"✅ Preprocessing params saved to preprocessing_params.json\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save preprocessing params: {e}\")\n",
    "    print(\"You may need to generate these from the original notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: TEST TFLITE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 8: TESTING TFLITE MODEL ===\")\n",
    "\n",
    "def test_tflite_model(tflite_path, test_input):\n",
    "    \"\"\"\n",
    "    Test TFLite model inference\n",
    "    \"\"\"\n",
    "    # Load TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input/output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Prepare input\n",
    "    input_data = test_input.astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get output\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data\n",
    "\n",
    "# Test dengan 5 samples\n",
    "print(\"\\nTesting TFLite model with sample inputs:\")\n",
    "test_samples = X_test_dense[:5]\n",
    "tflite_predictions = test_tflite_model('healthbot_classifier.tflite', test_samples)\n",
    "\n",
    "for i in range(5):\n",
    "    original_pred = y_pred_proba[i]\n",
    "    tflite_pred = tflite_predictions[i]\n",
    "\n",
    "    # Calculate difference\n",
    "    diff = np.mean(np.abs(original_pred - tflite_pred))\n",
    "    print(f\"Sample {i}: Avg difference = {diff:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: PREPARE FILES FOR ANDROID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 9: PREPARING ANDROID ASSETS ===\")\n",
    "\n",
    "# Create android_assets directory\n",
    "assets_dir = 'android_assets'\n",
    "if os.path.exists(assets_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(assets_dir)\n",
    "os.makedirs(assets_dir)\n",
    "\n",
    "# Copy files ke android_assets\n",
    "import shutil\n",
    "files_to_copy = [\n",
    "    ('healthbot_classifier.tflite', 'healthbot_classifier.tflite'),\n",
    "    ('label_mapping.json', 'label_mapping.json'),\n",
    "    ('preprocessing_params.json', 'preprocessing_params.json')\n",
    "]\n",
    "\n",
    "for src, dst in files_to_copy:\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, os.path.join(assets_dir, dst))\n",
    "        print(f\"✅ Copied {src} to {assets_dir}/{dst}\")\n",
    "    else:\n",
    "        print(f\"⚠️ File not found: {src}\")\n",
    "\n",
    "print(f\"\\n✅ Files prepared in {assets_dir}/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists('healthbot_classifier.tflite'):\n",
    "    model_size = os.path.getsize('healthbot_classifier.tflite') / (1024 * 1024)\n",
    "    print(f\"✅ Neural Network Model: Trained with F1-score: {f1_macro:.4f}\")\n",
    "    print(f\"✅ TensorFlow SavedModel: {saved_model_dir}\")\n",
    "    print(f\"✅ TensorFlow Lite Model: healthbot_classifier.tflite\")\n",
    "    print(f\"✅ Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"✅ Label Mapping: {len(label_mapping['labels'])} categories\")\n",
    "    print(f\"✅ Android Assets: {assets_dir}/\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Copy files from android_assets/ to your Android project\")\n",
    "    print(\"2. Implement TensorFlow Lite interpreter in Kotlin\")\n",
    "    print(\"3. Add preprocessing pipeline in Android\")\n",
    "    print(\"4. Test with sample data\")\n",
    "else:\n",
    "    print(\"⚠️ Model conversion failed. Please check the errors above.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}